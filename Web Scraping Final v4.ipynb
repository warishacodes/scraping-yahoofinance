{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm using BeautifulSoup for scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import os.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Url for CNN Moneyâ€™s Market Movers website\n",
    "\n",
    "url = \"https://money.cnn.com/data/hotstocks/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the HTML of URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Get the HTML of the above URL using requests.get(url) method.\n",
    "#If print statement is uncommented and run you can see html of website\n",
    "\n",
    "r = requests.get(url)\n",
    "htmlContent = r.content\n",
    "\n",
    "#print(htmlContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Parse the HTML using html parser\n",
    "\n",
    "soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "\n",
    "# print(soup.prettify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'GE', 'BAC', 'XOM', 'FCX', 'PFE', 'CCL', 'T', 'WFC', 'BSX']\n"
     ]
    }
   ],
   "source": [
    "#Step 3: HTML Tree Traversal\n",
    "#Since all of the tickers are contained inside of a table's anchor tag, I'm extracting tickers and appending them in a list\n",
    "#This is all we need to extract from CNN's website\n",
    "\n",
    "tbody = soup.find('table')\n",
    "anchor = tbody.find_all('a')\n",
    "tickers = []\n",
    "for a in anchor:\n",
    "    tickers.append(a.text)\n",
    "\n",
    "print(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL Concatenation for next website to be scraped\n",
    "#We need to change the URL to open the website for each ticker in tickers\n",
    "\n",
    "first_part = \"https://finance.yahoo.com/quote/\";\n",
    "third_part = \"?p=\";\n",
    "fifth_part = \"&.tsrc=fin-srch-v1\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data on each ticker from Yahoo Finance and file writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stocksdb will append all information about each stock in the order with which it will be stored in database. \n",
    "stockdb = []\n",
    "\n",
    "#records will append all information about each stock in the order with which it will be stored in stocks.txt.\n",
    "records = []\n",
    "\n",
    "#Loop for each ticker in tickers\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        #append yahoo finance url for each ticker\n",
    "        yahoo_url = first_part + ticker + third_part + ticker + fifth_part;\n",
    "        r2 = requests.get(yahoo_url)\n",
    "        htmlContent2 = r2.content\n",
    "        soup2 = BeautifulSoup(htmlContent2, 'html.parser')\n",
    "\n",
    "        #Open price\n",
    "        all_open_price = soup2.select(\"span[data-reactid*='103']\")[0].text\n",
    "\n",
    "        #Avg_volume and avg_volume value with comma removed so it can be stored as integer type\n",
    "        all_avg_volume = soup2.find('span', string = 'Avg. Volume').find_next().find_next().text\n",
    "        avg_volume_without_commas = all_avg_volume.replace(',', '')\n",
    "\n",
    "        #PE_ratio value is 0 where N/A\n",
    "        all_PE_ratio = soup2.find('span', string = 'PE Ratio (TTM)').find_next().find_next().text\n",
    "        if(all_PE_ratio == \"N/A\"):\n",
    "            all_PE_ratio = \"0\";\n",
    "\n",
    "        #appending it in the manner I want for the stocks.txt file (commas and new line)\n",
    "        stock_info = ticker + \",\" + all_open_price + \",\" + avg_volume_without_commas + \",\" + all_PE_ratio +','+'\\n'\n",
    "        records.append(stock_info)\n",
    "\n",
    "        #appending it in the manner I want for the database\n",
    "        stock = (ticker,all_open_price,avg_volume_without_commas,all_PE_ratio)\n",
    "        stockdb.append(stock)\n",
    "    except(AttributeError, KeyError) as er:\n",
    "        pass\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "#opening and writing to a file (a new file is made oneach execution)\n",
    "file = open('stocks.txt','w')\n",
    "line = \"\".join(records)\n",
    "file.write(line)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if DB already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if a DB already exists it will delete it and make a new one. Each execution of the program makes a new database.\n",
    "try:\n",
    "    checkDB = os.path.isfile('./StocksDatabase.db')\n",
    "    if(checkDB==True):\n",
    "        os.remove('StocksDatabase.db')\n",
    "except:\n",
    "    print(\"error: database already exists, go ahead\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting and creating database table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to a database\n",
    "conn = sqlite3.connect('StocksDatabase.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table\n",
    "try:\n",
    "    c.execute('''CREATE TABLE StocksTable\n",
    "             (Ticker text, OpenPrice real, AvgVolume integer, PERatio real)''')\n",
    "except: \n",
    "    print(\"error: table already exists, go ahead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1e25e68e340>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.executemany('INSERT INTO StocksTable VALUES (?,?,?,?)', stockdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "c.close()\n",
    "#Checked with SQlite, all data present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
